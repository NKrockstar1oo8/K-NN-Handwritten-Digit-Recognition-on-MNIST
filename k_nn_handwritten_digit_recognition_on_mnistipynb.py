# -*- coding: utf-8 -*-
"""k-NN Handwritten Digit Recognition on MNISTipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sfyo7enK8ZuPYNvH1JY2xY-OX0pOn0qr
"""

pip install numpy scikit-learn matplotlib seaborn

"""Setup and Data Acquisition"""

import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_openml
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split

print("--- Step 1: Library Setup and Data Acquisition ---")

# 1. Load the MNIST Dataset (70,000 images, 28x28 pixels)
# This may take a moment the very first time it runs.
print("1. Loading MNIST data...")
mnist = fetch_openml('mnist_784', version=1, cache=True)

# Separate features (X) and labels (y)
X, y = mnist["data"].values, mnist["target"].astype(np.uint8).values
print(f"Data loaded: {X.shape[0]} total samples.")
print(f"Each image is a {X.shape[1]}-dimensional vector (28x28=784 pixels).")

# Quick function to visualize a digit
def plot_digit(data, title=""):
    # Reshape the 784-vector back to a 28x28 image
    image = data.reshape(28, 28)
    plt.imshow(image, cmap="binary")
    plt.axis("off")
    plt.title(title)

# Display a sample digit
plt.figure(figsize=(2, 2))
plot_digit(X[0], title="Sample Digit (5)")
plt.show()

"""Preprocessing and Feature Vector Creation"""

print("--- Step 2: Preprocessing and Data Splitting ---")

# 1. Normalization (Scaling pixel values from 0-255 to 0-1)
# This improves the performance and stability of distance calculations.
X_normalized = X / 255.0
print("1. Data Normalized: Pixel values scaled to [0, 1].")

# 2. Split Data into Training and Testing Sets
# 60,000 samples for Training, 10,000 for Testing
X_train, X_test = X_normalized[:60000], X_normalized[60000:]
y_train, y_test = y[:60000], y[60000:]
print(f"2. Data Split: Training Set ({X_train.shape[0]}) | Test Set ({X_test.shape[0]})")

# 3. CRITICAL for quick demo: Reduce Training Sample Size
# k-NN is slow with large data; using a subset for fast results.
sample_size = 5000
X_train_sample = X_train[:sample_size]
y_train_sample = y_train[:sample_size]
print(f"3. Using a reduced training sample of {sample_size} images for speed.")

"""Model Training"""

# Model Selection and Training (k-NN)

k = 5
print(f"--- Step 3: Model Training (k-NN, k={k}) ---")
print("k-NN is a 'lazy learner'; training means storing the data and setting up the distance measure.")

# Initialize the k-NN classifier
# n_neighbors is the 'k' value (number of closest neighbors to check)
knn_clf = KNeighborsClassifier(n_neighbors=k, n_jobs=-1) # n_jobs=-1 uses all CPU cores

# 'Fit' the model (store the feature vectors and labels)
start_time = time.time()
knn_clf.fit(X_train_sample, y_train_sample)
end_time = time.time()

print(f"Model Training/Setup complete in: {end_time - start_time:.2f} seconds.")

"""Classification (Testing)"""

# Cell 4: Pattern Classification (Testing)

print("--- Step 4: Pattern Classification (Testing) ---")
print(f"Classifying the 10,000 images in the test set using distance metrics...")

start_time = time.time()
# Predict the class (digit) for every image in the test set
y_pred = knn_clf.predict(X_test)
end_time = time.time()

print(f"Prediction complete in: {end_time - start_time:.2f} seconds.")

# Calculate and display Overall Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy (k-NN, k={k}): {accuracy*100:.2f}%")

"""Evaluation and Analysis"""

# Cell 5: Evaluation and Analysis (Confusion Matrix)

print("--- Step 5: Evaluation and Error Analysis ---")

# 1. Generate Confusion Matrix
conf_mx = confusion_matrix(y_test, y_pred)

# 2. Visualization (Confusion Matrix & Sample Result)
plt.figure(figsize=(15, 6))

# Plot A: Confusion Matrix Heatmap
plt.subplot(1, 2, 1)
sns.heatmap(conf_mx, annot=True, fmt='d', cmap="Blues", cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title(f'Confusion Matrix (Accuracy: {accuracy*100:.2f}%)')
print("Diagonal = Correct Classifications. Off-diagonal = Misclassified Patterns.")

# Plot B: Visualize a specific test case
# Choose an index to display (e.g., the 9th test image)
test_index = 8
predicted_label = y_pred[test_index]
actual_label = y_test[test_index]

plt.subplot(1, 2, 2)
plot_digit(X_test[test_index],
           title=f"Actual: {actual_label} | Predicted: {predicted_label}")

plt.suptitle("Pattern Recognition Results: k-NN on MNIST Dataset")
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()

# Find an example of a MISCLASSIFIED image (for deeper analysis)
misclassified_index = np.where(y_test != y_pred)[0][0] # Find the index of the first error
misclassified_actual = y_test[misclassified_index]
misclassified_predicted = y_pred[misclassified_index]

plt.figure(figsize=(3, 3))
plot_digit(X_test[misclassified_index],
           title=f"ERROR: Actual {misclassified_actual} | Predicted {misclassified_predicted}")
plt.show()
print(f"Example of Misclassification: An image that was actually a '{misclassified_actual}' was classified as a '{misclassified_predicted}'.")